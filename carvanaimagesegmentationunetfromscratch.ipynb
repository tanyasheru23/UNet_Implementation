{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07469b6d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-06T14:19:03.329992Z",
     "iopub.status.busy": "2024-04-06T14:19:03.329000Z",
     "iopub.status.idle": "2024-04-06T14:19:10.281053Z",
     "shell.execute_reply": "2024-04-06T14:19:10.280114Z"
    },
    "papermill": {
     "duration": 6.975829,
     "end_time": "2024-04-06T14:19:10.299359",
     "exception": false,
     "start_time": "2024-04-06T14:19:03.323530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/unetcarvanadataset/data/train_masks/85bc655a7523_11_mask.gif\n",
      "/kaggle/input/unetcarvanadataset/data/train/b24fd9084449_06.jpg\n",
      "/kaggle/input/unetcarvanadataset/data/val_masks/0d53224da2b7_07_mask.gif\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e7c655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T14:19:10.344405Z",
     "iopub.status.busy": "2024-04-06T14:19:10.343982Z",
     "iopub.status.idle": "2024-04-06T14:19:16.125765Z",
     "shell.execute_reply": "2024-04-06T14:19:16.124946Z"
    },
    "papermill": {
     "duration": 5.803425,
     "end_time": "2024-04-06T14:19:16.128329",
     "exception": false,
     "start_time": "2024-04-06T14:19:10.324904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "    \n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.down_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        # self.max_pool = nn.MaxPool2d(2, 2)\n",
    "    def forward(self, X):\n",
    "        return self.down_layer(X)\n",
    "    \n",
    "class BottleNeck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(BottleNeck, self).__init__()\n",
    "        self.down_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        return self.down_layer(X)\n",
    "    \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Up, self).__init__()\n",
    "        # eg in_channels = 1024, out_channels = 512\n",
    "        # 1024 in_channels from prev layer\n",
    "        '''\n",
    "        In ConvTrans : 1024 --> 512\n",
    "        through skip_connection's 512 : 512 + 512 = 1024 (which is equal to in_channels)\n",
    "        So input of DoubleConv is also in_channels\n",
    "        In DoubleConv:\n",
    "            Conv2D: 1024 --> 512\n",
    "            Conv2D:  512 --> 512\n",
    "        \n",
    "        In ConvTrans : N --> N/2\n",
    "        through skip_connection's N/2 : N/2 + N/2 = N (which is equal to in_channels)\n",
    "        So input of DoubleConv is also in_channels\n",
    "        In DoubleConv:\n",
    "            Conv2D: N   --> N/2\n",
    "            Conv2D: N/2 --> N/2\n",
    "        for next layer, it will go from N/2 --> N/4\n",
    "        '''\n",
    "        self.up_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2) # 1024 --> 512\n",
    "        self.DoubleConv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False),  # 1024 --> 512\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False), # 1024 --> 512\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X, skip_connection):\n",
    "        X1 = self.up_conv(X)\n",
    "        if(X1.shape != skip_connection.shape):\n",
    "            X1 = TF.resize(X1, skip_connection.shape[2:]) # X1 height and width might not remain still same if max_pooling floors the dimension, so match it with the skip_connection height and width\n",
    "                \n",
    "        X2 = torch.cat((X1, skip_connection), dim=1) # concatenate skip_connection along channel dimension\n",
    "        \n",
    "        return self.DoubleConv(X2)\n",
    "\n",
    "class FinalConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(FinalConv, self).__init__()\n",
    "        self.finalConv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "        # kernel size = 1, since the height and width of the final layer and this should be same(given in paper)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.finalConv(X)\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels = 3, out_channels = 1):\n",
    "        super(UNet, self).__init__()\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.down1 = Down(in_channels, 64)\n",
    "        self.down2 = Down(64, 128)\n",
    "        self.down3 = Down(128, 256)\n",
    "        self.down4 = Down(256, 512)\n",
    "        \n",
    "        self.bottleNeck = Down(512, 1024) \n",
    "        \n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        \n",
    "        self.finalConv = FinalConv(64, out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        ### DownSampling\n",
    "        x1_skip = self.down1(X)          # 003-->064\n",
    "        x1 = self.max_pool(x1_skip)\n",
    "        \n",
    "        x2_skip = self.down2(x1)         # 064->128\n",
    "        x2 = self.max_pool(x2_skip)\n",
    "        \n",
    "        x3_skip = self.down3(x2)         # 128-->256\n",
    "        x3 = self.max_pool(x3_skip)\n",
    "        \n",
    "        x4_skip = self.down4(x3)         # 256-->512\n",
    "        x4 = self.max_pool(x4_skip)\n",
    "        \n",
    "        \n",
    "        ### BottleNeck Layer\n",
    "        x5 = self.bottleNeck(x4)         # 512-->1024\n",
    "        \n",
    "        \n",
    "        ### UpSampling        \n",
    "        x  = self.up1(x5, x4_skip)       # [x5(1024, up_conv will take it to 512) + x4(512)] --> 512\n",
    "        \n",
    "        x  = self.up2(x , x3_skip)       # [x ( 512, up_conv will take it to 256) + x3(256)] --> 256\n",
    "        \n",
    "        x  = self.up3(x , x2_skip)       # [x ( 256, up_conv will take it to 128) + x2(128)] --> 128\n",
    "        \n",
    "        x  = self.up4(x , x1_skip)       # [x ( 128, up_conv will take it to  64) + x1( 64)] --> 64\n",
    "        \n",
    "        x  = self.finalConv(x)           # 64 --> 2\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# def test():\n",
    "#     x = torch.randn(3, 1, 572, 572)\n",
    "#     model = UNet(in_channels=1, out_channels=1)\n",
    "#     preds = model(x)\n",
    "#     print(f\"Preds shape: {preds.shape}\")\n",
    "#     print(x.shape == preds.shape)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a6744e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T14:19:16.160563Z",
     "iopub.status.busy": "2024-04-06T14:19:16.160109Z",
     "iopub.status.idle": "2024-04-06T14:19:16.169433Z",
     "shell.execute_reply": "2024-04-06T14:19:16.168609Z"
    },
    "papermill": {
     "duration": 0.027186,
     "end_time": "2024-04-06T14:19:16.171343",
     "exception": false,
     "start_time": "2024-04-06T14:19:16.144157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "class CarvanaDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform = None):\n",
    "        super().__init__()\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(image_dir)\n",
    "        self.masks = os.listdir(mask_dir)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, key: int):\n",
    "        image_path = os.path.join(self.image_dir, self.images[key])\n",
    "        # mask_path = os.path.join(self.mask_dir, self.masks[key])\n",
    "        mask_path = os.path.join(self.mask_dir, self.images[key].replace(\".jpg\", \"_mask.gif\"))\n",
    "        image = np.array(Image.open(image_path).convert(\"RGB\")) # we are using np array since we will be using Albumentations library which req np array\n",
    "        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n",
    "        mask[mask == 255.0] = 1.0\n",
    "        \n",
    "        if self.transform:\n",
    "            augmentations = self.transform(image = image, mask = mask)\n",
    "            image = augmentations[\"image\"]\n",
    "            mask = augmentations[\"mask\"]\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "136998d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T14:19:16.202931Z",
     "iopub.status.busy": "2024-04-06T14:19:16.202628Z",
     "iopub.status.idle": "2024-04-06T14:19:16.216293Z",
     "shell.execute_reply": "2024-04-06T14:19:16.215490Z"
    },
    "papermill": {
     "duration": 0.03176,
     "end_time": "2024-04-06T14:19:16.218117",
     "exception": false,
     "start_time": "2024-04-06T14:19:16.186357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "# from dataset import CarvanaDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "\n",
    "def get_loaders(\n",
    "    train_dir,\n",
    "    train_maskdir,\n",
    "    val_dir, \n",
    "    val_maskdir,\n",
    "    train_transform,\n",
    "    val_transform,\n",
    "    batch_size,\n",
    "    num_workers,\n",
    "    pin_memory = True\n",
    "):\n",
    "    train_data = CarvanaDataset(image_dir=train_dir,\n",
    "                                mask_dir=train_maskdir, \n",
    "                                transform=train_transform)\n",
    "    val_data = CarvanaDataset(image_dir=val_dir, \n",
    "                                mask_dir=val_maskdir, \n",
    "                                transform=val_transform)\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=train_data,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=True,\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=pin_memory)\n",
    "    val_dataloader = DataLoader(dataset=val_data,\n",
    "                                batch_size=batch_size,\n",
    "                                shuffle=False,\n",
    "                                num_workers=num_workers,\n",
    "                                pin_memory=pin_memory)\n",
    "    \n",
    "    return train_dataloader, val_dataloader\n",
    "\n",
    "def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n",
    "    print(\"==> Saving CheckPoint\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(checkpoint, model):\n",
    "    print(\"==> Loading CheckPoint\")\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    \n",
    "def check_accuracy(loader, model, device):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for X, y in loader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device).unsqueeze(1)\n",
    "            preds = torch.sigmoid(model(X))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2*(preds*y).sum())/((preds + y).sum()+ 1e-8)\n",
    "    print(\n",
    "        f\"Got {num_correct}/{num_pixels} with accuracy {(num_correct/num_pixels)*100: .3f}\"\n",
    "    )\n",
    "    print(f\"Dice score: {dice_score/len(loader)}\")\n",
    "    model.train()\n",
    "    \n",
    "def save_predictions_as_imgs(loader, model,device, folder_dir = \"saved_images/\"):\n",
    "    model.eval()\n",
    "    \n",
    "    for idx, (X, y) in enumerate(loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        with torch.inference_mode():\n",
    "            preds = torch.sigmoid(model(X))\n",
    "            preds = (preds > 0.5).float()\n",
    "            \n",
    "        torchvision.utils.save_image(preds, f\"{folder_dir}/pred_{idx}.png\")\n",
    "        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder_dir}{idx}.png\")\n",
    "    \n",
    "    model.train()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84b46e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-06T14:19:16.250764Z",
     "iopub.status.busy": "2024-04-06T14:19:16.250012Z",
     "iopub.status.idle": "2024-04-06T14:40:27.708837Z",
     "shell.execute_reply": "2024-04-06T14:40:27.707766Z"
    },
    "papermill": {
     "duration": 1271.477907,
     "end_time": "2024-04-06T14:40:27.711222",
     "exception": false,
     "start_time": "2024-04-06T14:19:16.233315",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------Epoch: 0------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [02:04<00:00,  1.25it/s, loss=0.591]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 2733831/3686400 with accuracy  74.160\n",
      "Dice score: 0.605850100517273\n",
      "------Epoch: 1------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:42<00:00,  1.52it/s, loss=0.495]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3090293/3686400 with accuracy  83.830\n",
      "Dice score: 0.7182074785232544\n",
      "------Epoch: 2------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:35<00:00,  1.63it/s, loss=0.428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3243376/3686400 with accuracy  87.982\n",
      "Dice score: 0.7747312784194946\n",
      "------Epoch: 3------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:36<00:00,  1.62it/s, loss=0.39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3359350/3686400 with accuracy  91.128\n",
      "Dice score: 0.8236247897148132\n",
      "------Epoch: 4------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:37<00:00,  1.61it/s, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3459476/3686400 with accuracy  93.844\n",
      "Dice score: 0.870692253112793\n",
      "------Epoch: 5------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:36<00:00,  1.61it/s, loss=0.333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3507052/3686400 with accuracy  95.135\n",
      "Dice score: 0.8950232863426208\n",
      "------Epoch: 6------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:35<00:00,  1.63it/s, loss=0.314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3557414/3686400 with accuracy  96.501\n",
      "Dice score: 0.9220823049545288\n",
      "------Epoch: 7------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:39<00:00,  1.58it/s, loss=0.303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3590995/3686400 with accuracy  97.412\n",
      "Dice score: 0.9410356879234314\n",
      "------Epoch: 8------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:35<00:00,  1.64it/s, loss=0.304]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3586713/3686400 with accuracy  97.296\n",
      "Dice score: 0.9386633634567261\n",
      "------Epoch: 9------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:37<00:00,  1.59it/s, loss=0.279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3590331/3686400 with accuracy  97.394\n",
      "Dice score: 0.9407061338424683\n",
      "------Epoch: 10------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:39<00:00,  1.57it/s, loss=0.272]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3593987/3686400 with accuracy  97.493\n",
      "Dice score: 0.9428857564926147\n",
      "------Epoch: 11------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:39<00:00,  1.56it/s, loss=0.265]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving CheckPoint\n",
      "Got 3613304/3686400 with accuracy  98.017\n",
      "Dice score: 0.9541701078414917\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "# from u_net_model import UNet\n",
    "# # from model import UNET as UNet\n",
    "# from torch.utils.data import DataLoader\n",
    "# from dataset import CarvanaDataset\n",
    "\n",
    "# from utils import(\n",
    "#     load_checkpoint,\n",
    "#     save_checkpoint,\n",
    "#     get_loaders,\n",
    "#     check_accuracy,\n",
    "#     save_predictions_as_imgs\n",
    "# )\n",
    "\n",
    "LEARNING_RATE = 1e-6\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 12\n",
    "NUM_WORKERS = 2\n",
    "IMAGE_HEIGHT = 160\n",
    "IMAGE_WIDTH = 240\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = False\n",
    "TRAIN_IMG_DIR = \"/kaggle/input/unetcarvanadataset/data/train\"\n",
    "TRAIN_MASK_DIR = \"/kaggle/input/unetcarvanadataset/data/train_masks\"\n",
    "VAL_IMG_DIR = \"/kaggle/input/unetcarvanadataset/data/val\"\n",
    "VAL_MASK_DIR = \"/kaggle/input/unetcarvanadataset/data/val_masks\"\n",
    "\n",
    "def train_fn(loader, model, optimizer, loss_fn, scaler):\n",
    "    loop = tqdm(loader)\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(loop):\n",
    "        data = data.to(device = DEVICE)\n",
    "        targets = targets.float().unsqueeze(1).to(device = DEVICE)\n",
    "        \n",
    "        # forward\n",
    "        model.train()\n",
    "        predictions = model(data)\n",
    "        loss = loss_fn(predictions, targets)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loop.set_postfix(loss = loss.item())        \n",
    "\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Rotate(limit=35, p=1.0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std = [1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std = [1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = UNet(in_channels=3, out_channels=1).to(device=DEVICE)\n",
    "loss_fn = nn.BCEWithLogitsLoss() # with logits because in our model we didn't perform sigmoid after finalConv\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train_loader, val_loader = get_loaders(train_dir=TRAIN_IMG_DIR,\n",
    "                                        train_maskdir=TRAIN_MASK_DIR,\n",
    "                                        val_dir=VAL_IMG_DIR,\n",
    "                                        val_maskdir=VAL_MASK_DIR,\n",
    "                                        train_transform=train_transform,\n",
    "                                        val_transform=val_transform,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        num_workers=NUM_WORKERS,\n",
    "                                        pin_memory=PIN_MEMORY)\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"------Epoch: {epoch}------\")\n",
    "\n",
    "    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n",
    "\n",
    "    # save the model\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict()\n",
    "    }\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    # check accuracy\n",
    "    check_accuracy(val_loader, model, device=DEVICE)\n",
    "\n",
    "    # print some examples to a folder\n",
    "    save_predictions_as_imgs(\n",
    "        val_loader,\n",
    "        model,\n",
    "        folder_dir=\"/kaggle/working/\",\n",
    "        device=DEVICE\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4743924,
     "sourceId": 8045392,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1289.801312,
   "end_time": "2024-04-06T14:40:30.491355",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-06T14:19:00.690043",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
