{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8044887,"sourceType":"datasetVersion","datasetId":4743559}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/unetfolder/U_Net/data'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-05-13T05:19:41.434867Z","iopub.execute_input":"2024-05-13T05:19:41.435156Z","iopub.status.idle":"2024-05-13T05:19:47.568884Z","shell.execute_reply.started":"2024-05-13T05:19:41.435132Z","shell.execute_reply":"2024-05-13T05:19:47.567784Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/unetfolder/U_Net/data/val/0d1a9caf4350_16.jpg\n...\n/kaggle/input/unetfolder/U_Net/data/train_masks/784ca55262c2_15_mask.gif\n...\n/kaggle/input/unetfolder/U_Net/data/train/f70052627830_06.jpg\n...\n/kaggle/input/unetfolder/U_Net/data/val_masks/0d53224da2b7_07_mask.gif\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms.functional as TF\n\n    \nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Down, self).__init__()\n        self.down_layer = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        # self.max_pool = nn.MaxPool2d(2, 2)\n    def forward(self, X):\n        return self.down_layer(X)\n    \nclass BottleNeck(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(BottleNeck, self).__init__()\n        self.down_layer = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n    def forward(self, X):\n        return self.down_layer(X)\n    \nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Up, self).__init__()\n        # eg in_channels = 1024, out_channels = 512\n        # 1024 in_channels from prev layer\n        '''\n        In ConvTrans : 1024 --> 512\n        through skip_connection's 512 : 512 + 512 = 1024 (which is equal to in_channels)\n        So input of DoubleConv is also in_channels\n        In DoubleConv:\n            Conv2D: 1024 --> 512\n            Conv2D:  512 --> 512\n        \n        In ConvTrans : N --> N/2\n        through skip_connection's N/2 : N/2 + N/2 = N (which is equal to in_channels)\n        So input of DoubleConv is also in_channels\n        In DoubleConv:\n            Conv2D: N   --> N/2\n            Conv2D: N/2 --> N/2\n        for next layer, it will go from N/2 --> N/4\n        '''\n        self.up_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2) # 1024 --> 512\n        self.DoubleConv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias = False),  # 1024 --> 512\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias = False), # 1024 --> 512\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n        \n    def forward(self, X, skip_connection):\n        X1 = self.up_conv(X)\n        if(X1.shape != skip_connection.shape):\n            X1 = TF.resize(X1, skip_connection.shape[2:]) # X1 height and width might not remain still same if max_pooling floors the dimension, so match it with the skip_connection height and width\n                \n        X2 = torch.cat((X1, skip_connection), dim=1) # concatenate skip_connection along channel dimension\n        \n        return self.DoubleConv(X2)\n\nclass FinalConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(FinalConv, self).__init__()\n        self.finalConv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n        # kernel size = 1, since the height and width of the final layer and this should be same(given in paper)\n        \n    def forward(self, X):\n        return self.finalConv(X)\n    \nclass UNet(nn.Module):\n    def __init__(self, in_channels = 3, out_channels = 1):\n        super(UNet, self).__init__()\n        self.max_pool = nn.MaxPool2d(2, 2)\n        self.down1 = Down(in_channels, 64)\n        self.down2 = Down(64, 128)\n        self.down3 = Down(128, 256)\n        self.down4 = Down(256, 512)\n        \n        self.bottleNeck = Down(512, 1024) \n        \n        self.up1 = Up(1024, 512)\n        self.up2 = Up(512, 256)\n        self.up3 = Up(256, 128)\n        self.up4 = Up(128, 64)\n        \n        self.finalConv = FinalConv(64, out_channels)\n        \n        \n    def forward(self, X):\n        \n        ### DownSampling\n        x1_skip = self.down1(X)          # 003-->064\n        x1 = self.max_pool(x1_skip)\n        \n        x2_skip = self.down2(x1)         # 064->128\n        x2 = self.max_pool(x2_skip)\n        \n        x3_skip = self.down3(x2)         # 128-->256\n        x3 = self.max_pool(x3_skip)\n        \n        x4_skip = self.down4(x3)         # 256-->512\n        x4 = self.max_pool(x4_skip)\n        \n        \n        ### BottleNeck Layer\n        x5 = self.bottleNeck(x4)         # 512-->1024\n        \n        \n        ### UpSampling        \n        x  = self.up1(x5, x4_skip)       # [x5(1024, up_conv will take it to 512) + x4(512)] --> 512\n        \n        x  = self.up2(x , x3_skip)       # [x ( 512, up_conv will take it to 256) + x3(256)] --> 256\n        \n        x  = self.up3(x , x2_skip)       # [x ( 256, up_conv will take it to 128) + x2(128)] --> 128\n        \n        x  = self.up4(x , x1_skip)       # [x ( 128, up_conv will take it to  64) + x1( 64)] --> 64\n        \n        x  = self.finalConv(x)           # 64 --> 2\n        \n        return x\n\n\n# def test():\n#     x = torch.randn(3, 1, 572, 572)\n#     model = UNet(in_channels=1, out_channels=1)\n#     preds = model(x)\n#     print(f\"Preds shape: {preds.shape}\")\n#     print(x.shape == preds.shape)\n\n# if __name__ == \"__main__\":\n#     test()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:19:47.570929Z","iopub.execute_input":"2024-05-13T05:19:47.571629Z","iopub.status.idle":"2024-05-13T05:19:53.920264Z","shell.execute_reply.started":"2024-05-13T05:19:47.571593Z","shell.execute_reply":"2024-05-13T05:19:53.919494Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nimport pathlib\n\nfrom torch.utils.data import Dataset\nimport numpy as np\n\nclass CarvanaDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform = None):\n        super().__init__()\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.images = os.listdir(image_dir)\n        self.masks = os.listdir(mask_dir)\n        \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, key: int):\n        image_path = os.path.join(self.image_dir, self.images[key])\n        # mask_path = os.path.join(self.mask_dir, self.masks[key])\n        mask_path = os.path.join(self.mask_dir, self.images[key].replace(\".jpg\", \"_mask.gif\"))\n        image = np.array(Image.open(image_path).convert(\"RGB\")) # we are using np array since we will be using Albumentations library which req np array\n        mask = np.array(Image.open(mask_path).convert(\"L\"), dtype=np.float32)\n        mask[mask == 255.0] = 1.0\n        \n        if self.transform:\n            augmentations = self.transform(image = image, mask = mask)\n            image = augmentations[\"image\"]\n            mask = augmentations[\"mask\"]\n        return image, mask","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:19:53.921439Z","iopub.execute_input":"2024-05-13T05:19:53.922018Z","iopub.status.idle":"2024-05-13T05:19:53.931754Z","shell.execute_reply.started":"2024-05-13T05:19:53.921982Z","shell.execute_reply":"2024-05-13T05:19:53.930890Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\n# from dataset import CarvanaDataset\nfrom torch.utils.data import DataLoader\nimport torchvision\n\ndef get_loaders(\n    train_dir,\n    train_maskdir,\n    val_dir, \n    val_maskdir,\n    train_transform,\n    val_transform,\n    batch_size,\n    num_workers,\n    pin_memory = True\n):\n    train_data = CarvanaDataset(image_dir=train_dir,\n                                mask_dir=train_maskdir, \n                                transform=train_transform)\n    val_data = CarvanaDataset(image_dir=val_dir, \n                                mask_dir=val_maskdir, \n                                transform=val_transform)\n    \n    train_dataloader = DataLoader(dataset=train_data,\n                                batch_size=batch_size,\n                                shuffle=True,\n                                num_workers=num_workers,\n                                pin_memory=pin_memory)\n    val_dataloader = DataLoader(dataset=val_data,\n                                batch_size=batch_size,\n                                shuffle=False,\n                                num_workers=num_workers,\n                                pin_memory=pin_memory)\n    \n    return train_dataloader, val_dataloader\n\ndef save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n    print(\"==> Saving CheckPoint\")\n    torch.save(state, filename)\n\ndef load_checkpoint(checkpoint, model):\n    print(\"==> Loading CheckPoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    \ndef check_accuracy(loader, model, device):\n    num_correct = 0\n    num_pixels = 0\n    dice_score = 0\n    model.eval()\n    \n    with torch.inference_mode():\n        for X, y in loader:\n            X = X.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(X))\n            preds = (preds > 0.5).float()\n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2*(preds*y).sum())/((preds + y).sum()+ 1e-8)\n    print(\n        f\"Got {num_correct}/{num_pixels} with accuracy {(num_correct/num_pixels)*100: .3f}\"\n    )\n    print(f\"Dice score: {dice_score/len(loader)}\")\n    model.train()\n    \ndef save_predictions_as_imgs(loader, model,device, folder_dir = \"saved_images/\"):\n    model.eval()\n    \n    for idx, (X, y) in enumerate(loader):\n        X, y = X.to(device), y.to(device)\n        with torch.inference_mode():\n            preds = torch.sigmoid(model(X))\n            preds = (preds > 0.5).float()\n            \n        torchvision.utils.save_image(preds, f\"{folder_dir}/pred_{idx}.png\")\n        torchvision.utils.save_image(y.unsqueeze(1), f\"{folder_dir}{idx}.png\")\n    \n    model.train()        ","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:19:53.933428Z","iopub.execute_input":"2024-05-13T05:19:53.933721Z","iopub.status.idle":"2024-05-13T05:19:53.949242Z","shell.execute_reply.started":"2024-05-13T05:19:53.933699Z","shell.execute_reply":"2024-05-13T05:19:53.948319Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.optim\n# from u_net_model import UNet\n# # from model import UNET as UNet\n# from torch.utils.data import DataLoader\n# from dataset import CarvanaDataset\n\n# from utils import(\n#     load_checkpoint,\n#     save_checkpoint,\n#     get_loaders,\n#     check_accuracy,\n#     save_predictions_as_imgs\n# )\n\nLEARNING_RATE = 1e-4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBATCH_SIZE = 32\nEPOCHS = 10\nNUM_WORKERS = 2\nIMAGE_HEIGHT = 240\nIMAGE_WIDTH = 240\nPIN_MEMORY = True\nLOAD_MODEL = False\nTRAIN_IMG_DIR = \"/kaggle/input/unetfolder/U_Net/data/train\"\nTRAIN_MASK_DIR = \"/kaggle/input/unetfolder/U_Net/data/train_masks\"\nVAL_IMG_DIR = \"/kaggle/input/unetfolder/U_Net/data/val\"\nVAL_MASK_DIR = \"/kaggle/input/unetfolder/U_Net/data/val_masks\"\n\ndef train_fn(loader, model, optimizer, loss_fn, scaler):\n    loop = tqdm(loader)\n    \n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(device = DEVICE)\n        targets = targets.float().unsqueeze(1).to(device = DEVICE)\n        \n        # forward\n        model.train()\n        predictions = model(data)\n        loss = loss_fn(predictions, targets)\n        \n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loop.set_postfix(loss = loss.item())        \n\n# def main():\ntrain_transform = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Rotate(limit=35, p=1.0),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.1),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std = [1.0, 1.0, 1.0],\n            max_pixel_value=255.0\n        ),\n        ToTensorV2()\n    ]\n)\n\nval_transform = A.Compose(\n    [\n        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n        A.Normalize(\n            mean=[0.0, 0.0, 0.0],\n            std = [1.0, 1.0, 1.0],\n            max_pixel_value=255.0\n        ),\n        ToTensorV2()\n    ]\n)\n\nmodel = UNet(in_channels=3, out_channels=1).to(device=DEVICE)\nloss_fn = nn.BCEWithLogitsLoss() # with logits because in our model we didn't perform sigmoid after finalConv\noptimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n\ntrain_loader, val_loader = get_loaders(train_dir=TRAIN_IMG_DIR,\n                                        train_maskdir=TRAIN_MASK_DIR,\n                                        val_dir=VAL_IMG_DIR,\n                                        val_maskdir=VAL_MASK_DIR,\n                                        train_transform=train_transform,\n                                        val_transform=val_transform,\n                                        batch_size=BATCH_SIZE,\n                                        num_workers=NUM_WORKERS,\n                                        pin_memory=PIN_MEMORY)\n\nif LOAD_MODEL:\n    load_checkpoint(torch.load(\"my_checkpoint.pth.tar\"), model)\nscaler = torch.cuda.amp.GradScaler()\n\nfor epoch in range(EPOCHS):\n    print(f\"------Epoch: {epoch}------\")\n\n    train_fn(train_loader, model, optimizer, loss_fn, scaler)\n\n    # save the model\n    checkpoint = {\n        \"state_dict\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict()\n    }\n    save_checkpoint(checkpoint)\n\n    # check accuracy\n    check_accuracy(val_loader, model, device=DEVICE)\n\n    # print some examples to a folder\n    save_predictions_as_imgs(\n        val_loader,\n        model,\n        folder_dir=\"/kaggle/working/\",\n        device=DEVICE\n    )\n","metadata":{"execution":{"iopub.status.busy":"2024-05-13T05:19:53.950373Z","iopub.execute_input":"2024-05-13T05:19:53.950644Z","iopub.status.idle":"2024-05-13T05:39:02.893178Z","shell.execute_reply.started":"2024-05-13T05:19:53.950622Z","shell.execute_reply":"2024-05-13T05:39:02.892076Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"------Epoch: 0------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:56<00:00,  1.33it/s, loss=0.218]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5473131/5529600 with accuracy  98.979\nDice score: 0.9758269190788269\n------Epoch: 1------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:47<00:00,  1.45it/s, loss=0.158]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5477535/5529600 with accuracy  99.058\nDice score: 0.9777584075927734\n------Epoch: 2------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:47<00:00,  1.45it/s, loss=0.122]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5494604/5529600 with accuracy  99.367\nDice score: 0.9848635196685791\n------Epoch: 3------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:48<00:00,  1.44it/s, loss=0.0958]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5501105/5529600 with accuracy  99.485\nDice score: 0.9876548051834106\n------Epoch: 4------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:47<00:00,  1.45it/s, loss=0.0755]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5503950/5529600 with accuracy  99.536\nDice score: 0.9888496398925781\n------Epoch: 5------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:47<00:00,  1.45it/s, loss=0.0627]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5504013/5529600 with accuracy  99.537\nDice score: 0.9888418912887573\n------Epoch: 6------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:47<00:00,  1.45it/s, loss=0.0531]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5501720/5529600 with accuracy  99.496\nDice score: 0.9879042506217957\n------Epoch: 7------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:48<00:00,  1.44it/s, loss=0.0436]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5503865/5529600 with accuracy  99.535\nDice score: 0.9888229370117188\n------Epoch: 8------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:47<00:00,  1.45it/s, loss=0.0415]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5500527/5529600 with accuracy  99.474\nDice score: 0.9874448776245117\n------Epoch: 9------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 156/156 [01:48<00:00,  1.44it/s, loss=0.0336]\n","output_type":"stream"},{"name":"stdout","text":"==> Saving CheckPoint\nGot 5507804/5529600 with accuracy  99.606\nDice score: 0.9905095100402832\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
